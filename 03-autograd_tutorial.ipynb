{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pDL] *",
      "language": "python",
      "name": "conda-env-pDL-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "03-autograd_tutorial.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGdt6IrmDD2b"
      },
      "source": [
        "# Autograd: automatic differentiation\n",
        "\n",
        "The ``autograd`` package provides automatic differentiation for all operations\n",
        "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
        "defined by how your code is run, and that every single iteration can be\n",
        "different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_gz48twDD2e"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ncAo58pDD2l"
      },
      "source": [
        "Create a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW8s39-wDD2l",
        "outputId": "10e54a32-f2f3-4459-e7fc-4bb3cc13d35e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Create a 2x2 tensor with gradient-accumulation capabilities\n",
        "x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
        "print(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyn8KokWDD2s"
      },
      "source": [
        "Do an operation on the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuRds3blDD2u",
        "outputId": "9d6d543b-c3be-41b9-a86d-eef390031b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Deduct 2 from all elements\n",
        "y = x - 2\n",
        "print(y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.,  0.],\n",
            "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1YQtXRoDD20"
      },
      "source": [
        "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aVNG9lYDD21",
        "outputId": "bb423f33-ba31-49be-e1f3-2efb6b074cda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SubBackward0 object at 0x7f98dfadf278>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMJ9fBa6DD26",
        "outputId": "f0de7862-d9cf-4d77-91c0-90a24f573f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# What's happening here?\n",
        "print(x.grad_fn)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TQG_0nZDD2_",
        "outputId": "bb338d1c-6da0-4bd3-cfec-17b27fca4ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's dig further...\n",
        "y.grad_fn"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SubBackward0 at 0x7f98dfadf4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObuzB-KpDD3D",
        "outputId": "557dfa49-1d4f-40da-9f8e-319246f9e753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y.grad_fn.next_functions[0][0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AccumulateGrad at 0x7f9895c03278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtDKBV9KDD3I",
        "outputId": "e2f824cd-9d68-461a-a198-45b515235d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "y.grad_fn.next_functions[0][0].variable"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiX05_SfDD3N",
        "outputId": "15f2d1f1-fd42-4599-aaed-1aaae8bf9bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Do more operations on y\n",
        "z = y * y * 3\n",
        "a = z.mean()  # average\n",
        "\n",
        "print(z)\n",
        "print(a)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 3.,  0.],\n",
            "        [ 3., 12.]], grad_fn=<MulBackward0>)\n",
            "tensor(4.5000, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw6Wi1oMDD3S",
        "outputId": "c476a871-8e32-4081-9e3e-06c5cc0a9e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Let's visualise the computational graph! (thks @szagoruyko)\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 20.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 7.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.6.0+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=e2527bbf9d4b196e3b3774350d829ebd593dbba50bbb743d8621ebe0e5193f49\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hwu_f85DD3W",
        "outputId": "54be5f87-9176-4a50-fc6c-6d8ad410c7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "make_dot(a)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f9895c03748>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"106pt\" height=\"271pt\"\n viewBox=\"0.00 0.00 106.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-267 102,-267 102,4 -4,4\"/>\n<!-- 140293324158680 -->\n<g id=\"node1\" class=\"node\">\n<title>140293324158680</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"98,-21 0,-21 0,0 98,0 98,-21\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 140293324157672 -->\n<g id=\"node2\" class=\"node\">\n<title>140293324157672</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94.5,-78 3.5,-78 3.5,-57 94.5,-57 94.5,-78\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140293324157672&#45;&gt;140293324158680 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140293324157672&#45;&gt;140293324158680</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49,-56.7787C49,-49.6134 49,-39.9517 49,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-31.1732 49,-21.1732 45.5001,-31.1732 52.5001,-31.1732\"/>\n</g>\n<!-- 140293324158848 -->\n<g id=\"node3\" class=\"node\">\n<title>140293324158848</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94.5,-135 3.5,-135 3.5,-114 94.5,-114 94.5,-135\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140293324158848&#45;&gt;140293324157672 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140293324158848&#45;&gt;140293324157672</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49,-113.7787C49,-106.6134 49,-96.9517 49,-88.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-88.1732 49,-78.1732 45.5001,-88.1732 52.5001,-88.1732\"/>\n</g>\n<!-- 140294564476072 -->\n<g id=\"node4\" class=\"node\">\n<title>140294564476072</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94,-192 4,-192 4,-171 94,-171 94,-192\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140294564476072&#45;&gt;140293324158848 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140294564476072&#45;&gt;140293324158848</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M43.5885,-170.7787C42.3317,-163.6134 41.9599,-153.9517 42.4733,-145.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"45.9738,-145.498 43.597,-135.1732 39.0164,-144.7267 45.9738,-145.498\"/>\n</g>\n<!-- 140294564476072&#45;&gt;140293324158848 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140294564476072&#45;&gt;140293324158848</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M54.4115,-170.7787C55.6683,-163.6134 56.0401,-153.9517 55.5267,-145.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"58.9836,-144.7267 54.403,-135.1732 52.0262,-145.498 58.9836,-144.7267\"/>\n</g>\n<!-- 140293324157560 -->\n<g id=\"node5\" class=\"node\">\n<title>140293324157560</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"76,-263 22,-263 22,-228 76,-228 76,-263\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n<!-- 140293324157560&#45;&gt;140294564476072 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140293324157560&#45;&gt;140294564476072</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49,-227.6724C49,-219.8405 49,-210.5893 49,-202.4323\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-202.2234 49,-192.2234 45.5001,-202.2235 52.5001,-202.2234\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngJYU82DDD3c"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "Let's backprop now `out.backward()` is equivalent to doing `out.backward(torch.tensor([1.0]))`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiA8cdb8DD3c",
        "outputId": "6aaf10bd-0145-4046-c1da-865700759dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# Backprop\n",
        "a.backward()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-decee06d7466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug2vXYH8DD3g"
      },
      "source": [
        "Print gradients $\\frac{\\text{d}a}{\\text{d}x}$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-yXYlfEDD3h",
        "outputId": "ec62aede-0884-4ef0-e009-e8f730020736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Compute it by hand BEFORE executing this\n",
        "print(x.grad)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.5000,  0.0000],\n",
            "        [ 1.5000,  3.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l42WjhpxDD3k"
      },
      "source": [
        "You can do many crazy things with autograd!\n",
        "> With Great *Flexibility* Comes Great Responsibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBLl2vY3DD3l",
        "outputId": "54ad3694-f960-4f0a-ce02-4a73833fe0ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Dynamic graphs!\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "i = 0\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "    i += 1\n",
        "print(y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-650.1104, 1401.7023,  465.6096], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNlxdAokDD3o",
        "outputId": "93ed4408-3054-49bb-ef43-1dad5248678f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If we don't run backward on a scalar we need to specify the grad_output\n",
        "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
        "y.backward(gradients)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGkRY38PDD3r",
        "outputId": "e26b1ca1-3763-44f1-8891-d21161d028d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# BEFORE executing this, can you tell what would you expect it to print?\n",
        "print(i)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0y06I2lDD3u"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCrp06tfDD3w"
      },
      "source": [
        "# This variable decides the tensor's range below\n",
        "n = 3"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5ds-IuaDD30",
        "outputId": "7ac793e8-9736-46ac-a813-9d61962e9326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Both x and w that allows gradient accumulation\n",
        "x = torch.arange(1., n + 1, requires_grad=True)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "z = w @ x\n",
        "z.backward()\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.])\n",
            "tensor([1., 2., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOlh4IMlDD33",
        "outputId": "3aa6b28a-2439-4fbb-a04f-fbc992cbdcaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Only w that allows gradient accumulation\n",
        "x = torch.arange(1., n + 1)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "z = w @ x\n",
        "z.backward()\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "tensor([1., 2., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiRKPvX0DD36",
        "outputId": "e7bf301e-5f3c-4368-a319-2c14c43969fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x = torch.arange(1., n + 1)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "\n",
        "# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\n",
        "with torch.no_grad():\n",
        "    z = w @ x\n",
        "\n",
        "try:\n",
        "    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\n",
        "except RuntimeError as e:\n",
        "    print('RuntimeError!!! >:[')\n",
        "    print(e)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RuntimeError!!! >:[\n",
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYWf3ITODD39"
      },
      "source": [
        "## More stuff\n",
        "\n",
        "Documentation of the automatic differentiation package is at\n",
        "http://pytorch.org/docs/autograd."
      ]
    }
  ]
}